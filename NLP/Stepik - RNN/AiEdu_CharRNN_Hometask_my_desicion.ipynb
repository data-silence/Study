{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8R8WgZceEk"
      },
      "source": [
        "# Домашнее задание\n",
        "\n",
        "В этом домашнем задании вы обучите рекуррентную сеть для генерации текстов в стиле Шекспира."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUOE2flceEl"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHfCDyzceEl"
      },
      "source": [
        "## Загрузим данные\n",
        "\n",
        "Загрузим текстовый файл с пьесами Шекспира."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/shakespeare.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXAxa--jPUSd",
        "outputId": "72a3d073-98ed-4910-9800-0b51dd246d7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-16 14:42:11--  https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-01-16 14:42:11 (90.4 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Эта задача аналогична задаче, разобранной на вебинаре, поэтому код мы вам не предоставляем, а предлагаем или написать с нуля, или воспользоваться кодом с вебинара.*"
      ],
      "metadata": {
        "id": "YkQhIGj-nzYx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b34kfqIOceEl"
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open(\"shakespeare.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text), type(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sClaCUX8udZC",
        "outputId": "6e05e689-c3ca-4d4a-a1dd-bf8b4f124304"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115394, str)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "metadata": {
        "id": "aKsTauequn9V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OATLjEi9vN3a",
        "outputId": "068d09a5-cec6-4ed9-b41a-0c7a97eee3d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq0oEb_HvV72",
        "outputId": "8f025609-3162-4cc7-ec0f-d062988e0e7f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44, 40, 22, 56, 33, 47, 54, 40, 33, 40,  4, 36, 16, 32,  3, 45, 36,\n",
              "        6, 13, 22, 36, 47, 49, 36, 47, 57, 22, 13, 37, 36, 36, 23, 47, 30,\n",
              "       16, 11, 47,  6, 10, 22, 33, 17, 36, 22,  1, 47, 17, 36, 30, 22, 47,\n",
              "       46, 36, 47, 56, 57, 36, 30, 24, 52,  3,  3, 27, 34, 34, 32,  3, 25,\n",
              "       57, 36, 30, 24,  1, 47, 56, 57, 36, 30, 24, 52,  3,  3, 44, 40, 22,\n",
              "       56, 33, 47, 54, 40, 33, 40,  4, 36, 16, 32,  3,  5, 13, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(enumerate([7,8,9])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NUBi9-Ovlwe",
        "outputId": "a113c5a1-d1cc-42f2-dd3e-1bbeccb7323d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 7, 1: 8, 2: 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azltQy-gceEl"
      },
      "source": [
        "## Предобработка данных\n",
        "\n",
        "Как можно видеть на изображении char-RNN выше, сеть ожидает **one-hot encoded** входа, что означает, что каждый символ преобразуется в целое число (через созданный словарь), а затем преобразуется в вектор-столбец, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте создадим для этого функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnahALhiceEl"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.0\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb-8qCBsxPeL",
        "outputId": "8e7f2902-3743-495c-9448-7866a3c9c89e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YyL91CuceEl"
      },
      "source": [
        "## Создаем мини-батчи (mini-batchs)\n",
        "\n",
        "\n",
        "Создатдим мини-батчи для обучения. На простом примере они будут выглядеть так:\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
        "<br>\n",
        "\n",
        "Возьмем закодированные символы (переданные как параметр `arr`) и разделим их на несколько последовательностей, заданных параметром `batch_size`. Каждая из наших последовательностей будет иметь длину `seq_length`.\n",
        "\n",
        "### Создани батчей\n",
        "\n",
        "**1. Первое, что нам нужно сделать, это отбросить часть текста, чтобы у нас были только полные мини-батчи**\n",
        "\n",
        "Каждый батч содержит $ N\\times M $ символов, где $ N $ - это размер батча (количество последовательностей в батче), а $ M $ - длина `seq_length` или количество шагов в последовательности. Затем, чтобы получить общее количество батчей $ K $, которое мы можем сделать из массива `arr`, нужно разделить длину `arr` на количество символов в батче. Когда мы узнаем количество батчей, можно получить общее количество символов, которые нужно сохранить, из `arr`: $ N * M * K $.\n",
        "\n",
        "**2. После этого нам нужно разделить `arr` на $N$ батчей**\n",
        "\n",
        "Это можно сделать с помощью `arr.reshape(size)`, где `size` - это кортеж, содержащий размеры измененного массива. Мы знаем, что нам нужно $ N $ последовательностей в батче, поэтому сделаем его размером первого измерения. Для второго измерения можем использовать «-1» в качестве заполнителя, он заполнит массив соответствующими данными. После этого должен остаться массив $N\\times(M * K)$.\n",
        "\n",
        "**3. Теперь, когда у нас есть этот массив, мы можем перебирать его, чтобы получить наши мини-батчи**\n",
        "\n",
        "Идея состоит в том, что каждая партия представляет собой окно $ N\\times M $ в массиве $ N\\times (M * K) $. Для каждого последующего батча окно перемещается на `seq_length`. Мы также хотим создать как входной, так и выходной массивы. Это окно можно сделать с помощью `range`, чтобы делать шаги размером `n_steps` от $ 0 $ до `arr.shape [1]`, общее количество токенов в каждой последовательности. Таким образом, целые числа, которые получены из диапазона, всегда указывают на начало батча, и каждое окно имеет ширину `seq_length`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(int_words, batch_size, seq_length):\n",
        "    \"\"\"\n",
        "    Generates batches from encoded sequence.\n",
        "    :param int_words: tensor of ints, which represents the text; shape: [batch_size, -1]\n",
        "    :param batch_size: number of sequences per batch\n",
        "    :param seq_length: number of encoded chars in a sequence\n",
        "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
        "    \"\"\"\n",
        "    # 1. Truncate text, so there are only full batches\n",
        "    window_size = seq_length + 1\n",
        "    batch_size_total = batch_size * window_size\n",
        "    n_batches = len(int_words) // batch_size_total\n",
        "    int_words = int_words[: n_batches * batch_size_total]\n",
        "\n",
        "    # 2. Reshape into batch_size rows\n",
        "    int_words = int_words.reshape((batch_size, -1))\n",
        "\n",
        "    # 3. Iterate through the text matrix\n",
        "    for position in range(0, int_words.shape[1], window_size):\n",
        "        x = int_words[:, position : position + window_size - 1]\n",
        "        y = int_words[:, position + 1 : position + window_size]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "FLBOH8ZQUl7B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t5ADMcVgzPfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9uKOvbqceEl"
      },
      "source": [
        "### Протестируем\n",
        "\n",
        "Теперь создадим несколько наборов данных, и проверим, что происходит, когда мы создаем батчи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtKlLXi1ceEl"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg5MUTqqceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c137799-074d-452e-b724-f52c6154b9fc"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print(\"x\\n\", x[:10, :10])\n",
        "print(\"\\ny\\n\", y[:10, :10])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[44 40 22 56 33 47 54 40 33 40]\n",
            " [17 47 33 17 11 47 41 40 37 33]\n",
            " [47 40 56 32  3 45 36 33 33 36]\n",
            " [30 16 47 30 34 36 17 13 10 56]\n",
            " [37 40 30 16 32  3 39 17 30 33]\n",
            " [30 22 21 36 18  3 39 36 47 49]\n",
            " [36 34 47 30 16 23 47 19 47 24]\n",
            " [47 33 17 30 33 47 13 16 37 36]]\n",
            "\n",
            "y\n",
            " [[40 22 56 33 47 54 40 33 40  4]\n",
            " [47 33 17 11 47 41 40 37 33 13]\n",
            " [40 56 32  3 45 36 33 33 36 22]\n",
            " [16 47 30 34 36 17 13 10 56 36]\n",
            " [40 30 16 32  3 39 17 30 33 47]\n",
            " [22 21 36 18  3 39 36 47 49 40]\n",
            " [34 47 30 16 23 47 19 47 24 16]\n",
            " [33 17 30 33 47 13 16 37 36 47]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jouxv0L2ceEl"
      },
      "source": [
        "---\n",
        "## Зададим архитектуру\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7s5eRaoceEl"
      },
      "source": [
        "### Структура модели\n",
        "\n",
        "В `__init__` предлагаемая структура выглядит следующим образом:\n",
        "* Создаваём и храним необходимые словари (уже релизовано)\n",
        "* Определяем слой LSTM, который принимает в качестве параметров: размер ввода (количество символов), размер скрытого слоя `n_hidden`, количество слоев` n_layers`, вероятность drop-out'а `drop_prob` и логическое значение batch_first (True)\n",
        "* Определяем слой drop-out с помощью drop_prob\n",
        "* Определяем полносвязанный слой с параметрами: размер ввода `n_hidden` и размер выхода - количество символов\n",
        "* Наконец, инициализируем веса\n",
        "\n",
        "Обратите внимание, что некоторые параметры были названы и указаны в функции `__init__`, их нужно сохранить и использовать, выполняя что-то вроде` self.drop_prob = drop_prob`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plm1atCuceEl"
      },
      "source": [
        "---\n",
        "### Входы-выходы LSTM\n",
        "\n",
        "Вы можете создать [LSTM layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) следующим образом\n",
        "\n",
        "```python\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "```\n",
        "\n",
        "где `input_siz`e - это количество символов, которые эта ячейка ожидает видеть в качестве последовательного ввода, а `n_hidden` - это количество элементов в скрытых слоях ячейки. Можно добавить drop-out, добавив параметр `dropout` с заданной вероятностью. Наконец, в функции `forward` мы можем складывать ячейки LSTM в слои, используя `.view`.\n",
        "\n",
        "Также требуется создать начальное скрытое состояние всех нулей:\n",
        "\n",
        "```python\n",
        "self.init_hidden()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vPDxne_N0Opq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlTnDntHceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f072ef51-e5e7-40f8-f215-d2bf327c1488"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"Training on GPU!\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU; consider making n_epochs very small.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPq1EA38rBqn"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state `hidden`.\"\"\"\n",
        "\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if train_on_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aqi76nOE3fEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IrBRlEPceEl"
      },
      "source": [
        "## Обучим модель\n",
        "\n",
        "Во время обучения нужно установить количество эпох, скорость обучения и другие параметры.\n",
        "\n",
        "Используем оптимизатор Adam и кросс-энтропию, считаем loss и, как обычно, выполняем back propagation.\n",
        "\n",
        "Пара подробностей об обучении:\n",
        "> * Во время цикла мы отделяем скрытое состояние от его истории; на этот раз установив его равным новой переменной * tuple *, потому что скрытое состояние LSTM, является кортежем скрытых состояний.\n",
        "* Мы используем [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) чтобы избавиться от взрывающихся градиентов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv8VkRI0ceEl"
      },
      "source": [
        "def train(\n",
        "    net,\n",
        "    data,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    seq_length=50,\n",
        "    lr=0.001,\n",
        "    clip=5,\n",
        "    val_frac=0.1,\n",
        "    print_every=10,\n",
        "):\n",
        "    \"\"\"Training a network\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "\n",
        "    net: CharRNN network\n",
        "    data: text data to train the network\n",
        "    epochs: Number of epochs to train\n",
        "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "    seq_length: Number of character steps per mini-batch\n",
        "    lr: learning rate\n",
        "    clip: gradient clipping\n",
        "    val_frac: Fraction of data to hold out for validation\n",
        "    print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if train_on_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if train_on_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output, targets.view(batch_size * seq_length).long()\n",
        "                    )\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\n",
        "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                )"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8t0QGomG4mew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt0q4KGEceEm"
      },
      "source": [
        "## Определим модель\n",
        "\n",
        "Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykMcIloEr3G7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc3cf3b2-1d10-4322-ca44-edf33ec37b17"
      },
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(65, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, layer in enumerate(net.parameters()):\n",
        "  print(i, layer.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEEMdanu5rM7",
        "outputId": "ec90b70b-02a2-4eb5-b9c0-0feef64a1996"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([2048, 65])\n",
            "1 torch.Size([2048, 512])\n",
            "2 torch.Size([2048])\n",
            "3 torch.Size([2048])\n",
            "4 torch.Size([2048, 512])\n",
            "5 torch.Size([2048, 512])\n",
            "6 torch.Size([2048])\n",
            "7 torch.Size([2048])\n",
            "8 torch.Size([65, 512])\n",
            "9 torch.Size([65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHy6mECuceEm"
      },
      "source": [
        "### Установим гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hTkNrWEsjgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c6dc03-cac0-4def-dc16-45a3ef4a6ab7"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20  # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net=net,\n",
        "    data=encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=0.001,\n",
        "    print_every=10,\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.3573... Val Loss: 3.3892\n",
            "Epoch: 1/20... Step: 20... Loss: 3.3215... Val Loss: 3.3481\n",
            "Epoch: 1/20... Step: 30... Loss: 3.3091... Val Loss: 3.3456\n",
            "Epoch: 1/20... Step: 40... Loss: 3.3358... Val Loss: 3.3357\n",
            "Epoch: 1/20... Step: 50... Loss: 3.3102... Val Loss: 3.3126\n",
            "Epoch: 1/20... Step: 60... Loss: 3.2412... Val Loss: 3.2610\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1970... Val Loss: 3.1825\n",
            "Epoch: 2/20... Step: 80... Loss: 3.1332... Val Loss: 3.1035\n",
            "Epoch: 2/20... Step: 90... Loss: 3.0536... Val Loss: 3.0161\n",
            "Epoch: 2/20... Step: 100... Loss: 2.9751... Val Loss: 2.9260\n",
            "Epoch: 2/20... Step: 110... Loss: 2.8259... Val Loss: 2.7652\n",
            "Epoch: 2/20... Step: 120... Loss: 2.7561... Val Loss: 2.7013\n",
            "Epoch: 2/20... Step: 130... Loss: 2.6321... Val Loss: 2.5856\n",
            "Epoch: 2/20... Step: 140... Loss: 2.5608... Val Loss: 2.5046\n",
            "Epoch: 2/20... Step: 150... Loss: 2.5032... Val Loss: 2.4502\n",
            "Epoch: 3/20... Step: 160... Loss: 2.4612... Val Loss: 2.4100\n",
            "Epoch: 3/20... Step: 170... Loss: 2.4405... Val Loss: 2.3714\n",
            "Epoch: 3/20... Step: 180... Loss: 2.4134... Val Loss: 2.3440\n",
            "Epoch: 3/20... Step: 190... Loss: 2.3609... Val Loss: 2.3237\n",
            "Epoch: 3/20... Step: 200... Loss: 2.3060... Val Loss: 2.2838\n",
            "Epoch: 3/20... Step: 210... Loss: 2.3231... Val Loss: 2.2624\n",
            "Epoch: 3/20... Step: 220... Loss: 2.2635... Val Loss: 2.2357\n",
            "Epoch: 3/20... Step: 230... Loss: 2.2480... Val Loss: 2.2142\n",
            "Epoch: 4/20... Step: 240... Loss: 2.2044... Val Loss: 2.1951\n",
            "Epoch: 4/20... Step: 250... Loss: 2.1978... Val Loss: 2.1677\n",
            "Epoch: 4/20... Step: 260... Loss: 2.2058... Val Loss: 2.1487\n",
            "Epoch: 4/20... Step: 270... Loss: 2.1724... Val Loss: 2.1316\n",
            "Epoch: 4/20... Step: 280... Loss: 2.1349... Val Loss: 2.1165\n",
            "Epoch: 4/20... Step: 290... Loss: 2.1220... Val Loss: 2.1023\n",
            "Epoch: 4/20... Step: 300... Loss: 2.0951... Val Loss: 2.0887\n",
            "Epoch: 5/20... Step: 310... Loss: 2.0862... Val Loss: 2.0697\n",
            "Epoch: 5/20... Step: 320... Loss: 2.0655... Val Loss: 2.0510\n",
            "Epoch: 5/20... Step: 330... Loss: 2.0509... Val Loss: 2.0361\n",
            "Epoch: 5/20... Step: 340... Loss: 2.0523... Val Loss: 2.0255\n",
            "Epoch: 5/20... Step: 350... Loss: 2.0277... Val Loss: 2.0175\n",
            "Epoch: 5/20... Step: 360... Loss: 2.0150... Val Loss: 2.0112\n",
            "Epoch: 5/20... Step: 370... Loss: 1.9881... Val Loss: 1.9981\n",
            "Epoch: 5/20... Step: 380... Loss: 1.9875... Val Loss: 1.9849\n",
            "Epoch: 6/20... Step: 390... Loss: 1.9527... Val Loss: 1.9746\n",
            "Epoch: 6/20... Step: 400... Loss: 1.9612... Val Loss: 1.9610\n",
            "Epoch: 6/20... Step: 410... Loss: 1.9305... Val Loss: 1.9507\n",
            "Epoch: 6/20... Step: 420... Loss: 1.9336... Val Loss: 1.9390\n",
            "Epoch: 6/20... Step: 430... Loss: 1.9015... Val Loss: 1.9324\n",
            "Epoch: 6/20... Step: 440... Loss: 1.9068... Val Loss: 1.9291\n",
            "Epoch: 6/20... Step: 450... Loss: 1.8900... Val Loss: 1.9191\n",
            "Epoch: 6/20... Step: 460... Loss: 1.8946... Val Loss: 1.9067\n",
            "Epoch: 7/20... Step: 470... Loss: 1.8984... Val Loss: 1.8993\n",
            "Epoch: 7/20... Step: 480... Loss: 1.8978... Val Loss: 1.8862\n",
            "Epoch: 7/20... Step: 490... Loss: 1.8481... Val Loss: 1.8799\n",
            "Epoch: 7/20... Step: 500... Loss: 1.8566... Val Loss: 1.8709\n",
            "Epoch: 7/20... Step: 510... Loss: 1.8465... Val Loss: 1.8675\n",
            "Epoch: 7/20... Step: 520... Loss: 1.8511... Val Loss: 1.8640\n",
            "Epoch: 7/20... Step: 530... Loss: 1.7884... Val Loss: 1.8541\n",
            "Epoch: 8/20... Step: 540... Loss: 1.8578... Val Loss: 1.8480\n",
            "Epoch: 8/20... Step: 550... Loss: 1.8270... Val Loss: 1.8403\n",
            "Epoch: 8/20... Step: 560... Loss: 1.7838... Val Loss: 1.8310\n",
            "Epoch: 8/20... Step: 570... Loss: 1.7783... Val Loss: 1.8217\n",
            "Epoch: 8/20... Step: 580... Loss: 1.7790... Val Loss: 1.8157\n",
            "Epoch: 8/20... Step: 590... Loss: 1.8000... Val Loss: 1.8129\n",
            "Epoch: 8/20... Step: 600... Loss: 1.7540... Val Loss: 1.8121\n",
            "Epoch: 8/20... Step: 610... Loss: 1.7653... Val Loss: 1.7976\n",
            "Epoch: 9/20... Step: 620... Loss: 1.7847... Val Loss: 1.7996\n",
            "Epoch: 9/20... Step: 630... Loss: 1.7719... Val Loss: 1.7937\n",
            "Epoch: 9/20... Step: 640... Loss: 1.7223... Val Loss: 1.7815\n",
            "Epoch: 9/20... Step: 650... Loss: 1.7379... Val Loss: 1.7750\n",
            "Epoch: 9/20... Step: 660... Loss: 1.7313... Val Loss: 1.7691\n",
            "Epoch: 9/20... Step: 670... Loss: 1.7404... Val Loss: 1.7632\n",
            "Epoch: 9/20... Step: 680... Loss: 1.7260... Val Loss: 1.7616\n",
            "Epoch: 9/20... Step: 690... Loss: 1.7178... Val Loss: 1.7560\n",
            "Epoch: 10/20... Step: 700... Loss: 1.7066... Val Loss: 1.7547\n",
            "Epoch: 10/20... Step: 710... Loss: 1.7169... Val Loss: 1.7489\n",
            "Epoch: 10/20... Step: 720... Loss: 1.7079... Val Loss: 1.7406\n",
            "Epoch: 10/20... Step: 730... Loss: 1.7212... Val Loss: 1.7341\n",
            "Epoch: 10/20... Step: 740... Loss: 1.6896... Val Loss: 1.7344\n",
            "Epoch: 10/20... Step: 750... Loss: 1.6721... Val Loss: 1.7291\n",
            "Epoch: 10/20... Step: 760... Loss: 1.6783... Val Loss: 1.7265\n",
            "Epoch: 10/20... Step: 770... Loss: 1.6697... Val Loss: 1.7202\n",
            "Epoch: 11/20... Step: 780... Loss: 1.7166... Val Loss: 1.7159\n",
            "Epoch: 11/20... Step: 790... Loss: 1.6943... Val Loss: 1.7101\n",
            "Epoch: 11/20... Step: 800... Loss: 1.6722... Val Loss: 1.7035\n",
            "Epoch: 11/20... Step: 810... Loss: 1.6510... Val Loss: 1.7003\n",
            "Epoch: 11/20... Step: 820... Loss: 1.6324... Val Loss: 1.6997\n",
            "Epoch: 11/20... Step: 830... Loss: 1.6524... Val Loss: 1.7043\n",
            "Epoch: 11/20... Step: 840... Loss: 1.6456... Val Loss: 1.6940\n",
            "Epoch: 12/20... Step: 850... Loss: 1.6780... Val Loss: 1.6888\n",
            "Epoch: 12/20... Step: 860... Loss: 1.6484... Val Loss: 1.6880\n",
            "Epoch: 12/20... Step: 870... Loss: 1.6364... Val Loss: 1.6810\n",
            "Epoch: 12/20... Step: 880... Loss: 1.6009... Val Loss: 1.6799\n",
            "Epoch: 12/20... Step: 890... Loss: 1.6187... Val Loss: 1.6746\n",
            "Epoch: 12/20... Step: 900... Loss: 1.6352... Val Loss: 1.6780\n",
            "Epoch: 12/20... Step: 910... Loss: 1.5959... Val Loss: 1.6728\n",
            "Epoch: 12/20... Step: 920... Loss: 1.6207... Val Loss: 1.6679\n",
            "Epoch: 13/20... Step: 930... Loss: 1.6061... Val Loss: 1.6693\n",
            "Epoch: 13/20... Step: 940... Loss: 1.6309... Val Loss: 1.6642\n",
            "Epoch: 13/20... Step: 950... Loss: 1.6025... Val Loss: 1.6578\n",
            "Epoch: 13/20... Step: 960... Loss: 1.5814... Val Loss: 1.6551\n",
            "Epoch: 13/20... Step: 970... Loss: 1.5829... Val Loss: 1.6526\n",
            "Epoch: 13/20... Step: 980... Loss: 1.5819... Val Loss: 1.6521\n",
            "Epoch: 13/20... Step: 990... Loss: 1.6058... Val Loss: 1.6503\n",
            "Epoch: 13/20... Step: 1000... Loss: 1.6063... Val Loss: 1.6480\n",
            "Epoch: 14/20... Step: 1010... Loss: 1.5792... Val Loss: 1.6447\n",
            "Epoch: 14/20... Step: 1020... Loss: 1.5868... Val Loss: 1.6448\n",
            "Epoch: 14/20... Step: 1030... Loss: 1.6231... Val Loss: 1.6348\n",
            "Epoch: 14/20... Step: 1040... Loss: 1.5780... Val Loss: 1.6342\n",
            "Epoch: 14/20... Step: 1050... Loss: 1.5733... Val Loss: 1.6361\n",
            "Epoch: 14/20... Step: 1060... Loss: 1.5903... Val Loss: 1.6368\n",
            "Epoch: 14/20... Step: 1070... Loss: 1.5797... Val Loss: 1.6296\n",
            "Epoch: 15/20... Step: 1080... Loss: 1.5693... Val Loss: 1.6326\n",
            "Epoch: 15/20... Step: 1090... Loss: 1.5660... Val Loss: 1.6254\n",
            "Epoch: 15/20... Step: 1100... Loss: 1.5379... Val Loss: 1.6254\n",
            "Epoch: 15/20... Step: 1110... Loss: 1.5655... Val Loss: 1.6214\n",
            "Epoch: 15/20... Step: 1120... Loss: 1.5562... Val Loss: 1.6189\n",
            "Epoch: 15/20... Step: 1130... Loss: 1.5620... Val Loss: 1.6229\n",
            "Epoch: 15/20... Step: 1140... Loss: 1.5440... Val Loss: 1.6145\n",
            "Epoch: 15/20... Step: 1150... Loss: 1.5542... Val Loss: 1.6139\n",
            "Epoch: 16/20... Step: 1160... Loss: 1.5384... Val Loss: 1.6130\n",
            "Epoch: 16/20... Step: 1170... Loss: 1.5480... Val Loss: 1.6117\n",
            "Epoch: 16/20... Step: 1180... Loss: 1.5427... Val Loss: 1.6080\n",
            "Epoch: 16/20... Step: 1190... Loss: 1.5312... Val Loss: 1.6052\n",
            "Epoch: 16/20... Step: 1200... Loss: 1.5063... Val Loss: 1.6037\n",
            "Epoch: 16/20... Step: 1210... Loss: 1.5267... Val Loss: 1.6069\n",
            "Epoch: 16/20... Step: 1220... Loss: 1.5457... Val Loss: 1.6065\n",
            "Epoch: 16/20... Step: 1230... Loss: 1.5483... Val Loss: 1.6008\n",
            "Epoch: 17/20... Step: 1240... Loss: 1.5429... Val Loss: 1.6000\n",
            "Epoch: 17/20... Step: 1250... Loss: 1.5661... Val Loss: 1.5958\n",
            "Epoch: 17/20... Step: 1260... Loss: 1.5171... Val Loss: 1.5939\n",
            "Epoch: 17/20... Step: 1270... Loss: 1.5203... Val Loss: 1.5920\n",
            "Epoch: 17/20... Step: 1280... Loss: 1.5287... Val Loss: 1.5932\n",
            "Epoch: 17/20... Step: 1290... Loss: 1.5300... Val Loss: 1.5948\n",
            "Epoch: 17/20... Step: 1300... Loss: 1.4727... Val Loss: 1.5903\n",
            "Epoch: 18/20... Step: 1310... Loss: 1.5446... Val Loss: 1.5878\n",
            "Epoch: 18/20... Step: 1320... Loss: 1.5448... Val Loss: 1.5876\n",
            "Epoch: 18/20... Step: 1330... Loss: 1.4957... Val Loss: 1.5845\n",
            "Epoch: 18/20... Step: 1340... Loss: 1.5021... Val Loss: 1.5822\n",
            "Epoch: 18/20... Step: 1350... Loss: 1.4996... Val Loss: 1.5811\n",
            "Epoch: 18/20... Step: 1360... Loss: 1.5223... Val Loss: 1.5812\n",
            "Epoch: 18/20... Step: 1370... Loss: 1.4985... Val Loss: 1.5786\n",
            "Epoch: 18/20... Step: 1380... Loss: 1.5223... Val Loss: 1.5820\n",
            "Epoch: 19/20... Step: 1390... Loss: 1.5216... Val Loss: 1.5784\n",
            "Epoch: 19/20... Step: 1400... Loss: 1.5158... Val Loss: 1.5754\n",
            "Epoch: 19/20... Step: 1410... Loss: 1.4664... Val Loss: 1.5733\n",
            "Epoch: 19/20... Step: 1420... Loss: 1.4985... Val Loss: 1.5707\n",
            "Epoch: 19/20... Step: 1430... Loss: 1.4846... Val Loss: 1.5721\n",
            "Epoch: 19/20... Step: 1440... Loss: 1.5150... Val Loss: 1.5735\n",
            "Epoch: 19/20... Step: 1450... Loss: 1.4930... Val Loss: 1.5704\n",
            "Epoch: 19/20... Step: 1460... Loss: 1.4904... Val Loss: 1.5718\n",
            "Epoch: 20/20... Step: 1470... Loss: 1.4773... Val Loss: 1.5660\n",
            "Epoch: 20/20... Step: 1480... Loss: 1.4861... Val Loss: 1.5649\n",
            "Epoch: 20/20... Step: 1490... Loss: 1.4908... Val Loss: 1.5619\n",
            "Epoch: 20/20... Step: 1500... Loss: 1.5008... Val Loss: 1.5649\n",
            "Epoch: 20/20... Step: 1510... Loss: 1.4734... Val Loss: 1.5632\n",
            "Epoch: 20/20... Step: 1520... Loss: 1.4688... Val Loss: 1.5663\n",
            "Epoch: 20/20... Step: 1530... Loss: 1.4714... Val Loss: 1.5630\n",
            "Epoch: 20/20... Step: 1540... Loss: 1.4797... Val Loss: 1.5642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQiPmGH676j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfZxvNoDceEm"
      },
      "source": [
        "## Checkpoint\n",
        "\n",
        "После обучения сохраним модель, чтобы можно было загрузить ее позже. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6RXl5VAceEm"
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = \"rnn_x_epoch.net\"\n",
        "\n",
        "checkpoint = {\n",
        "    \"n_hidden\": net.n_hidden,\n",
        "    \"n_layers\": net.n_layers,\n",
        "    \"state_dict\": net.state_dict(),\n",
        "    \"tokens\": net.chars,\n",
        "}\n",
        "\n",
        "with open(model_name, \"wb\") as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2sJhx5iceEm"
      },
      "source": [
        "---\n",
        "## Делаем предсказания\n",
        "\n",
        "Теперь, когда мы обучили модель, сделаем предсказание следующих символов! Для предсказания мы передаем последний символ, и сеть предсказывает следующий символ, который мы потом передаем снова на вхол и получаем еще один предсказанный символ и так далее...\n",
        "\n",
        "Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые наиболее вероятные символы $K$. Это не позволит сети выдавать нам совершенно абсурдные прогнозы, а также позволит внести некоторый шум и случайность в выбранный текст. Узнать больше [можно здесь](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEIRW_B2ceEm"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \"\"\"Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return net.int2char[char], h"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FllUGcU8TbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG38j3gQceEm"
      },
      "source": [
        "### Priming и генерирование текста\n",
        "\n",
        "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9vpB5gRceEm"
      },
      "source": [
        "def sample(net, size, prime=\"The\", top_k=None):\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return \"\".join(chars)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, top_k=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQNLhbCs8blj",
        "outputId": "8f16928e-9cd3-41aa-c2dd-312242d10b90"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theme a man aro true that I shall.\n",
            "\n",
            "KING RICHARD III:\n",
            "Ay, brings your hands where'st he hours of thy lives.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Why, thou sheet so by; be to me, but the were,\n",
            "And which all hour and the pease is true sour,\n",
            "That ther we shall be sontial of his poor as thou\n",
            "constlunting, as I have see thee to this bed the sire\n",
            "And will be the way.\n",
            "\n",
            "PAULINA:\n",
            "Thy lord, the country to thee?\n",
            "\n",
            "CORIOLANUS:\n",
            "What is a prayer of the shall what we would tear.\n",
            "\n",
            "LADY ANNE:\n",
            "A presate of the prince, but there's a poor\n",
            "As husband and his bariage to such a merty\n",
            "But with her heavens thee, what wasto yourself there,\n",
            "And then, if he speaker with me to the palling, to his son?\n",
            "When you have do not, by at this some service shall\n",
            "I was the senting her then are at take of more;\n",
            "Fet, thone that senst to stay is see and stiff as\n",
            "to show their tales. To them to his fair of the duke.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Ay, sor of to a hory to be the person\n",
            "Whits and the trancours, and that the sun war stay\n",
            "By how is to the world.\n",
            "\n",
            "POMPEY:\n",
            "That's m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "supernet = CharRNN(chars, n_hidden, n_layers)"
      ],
      "metadata": {
        "id": "n1wyvADY8yaO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    net=supernet,\n",
        "    data=encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=1,\n",
        "    print_every=10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C0khCkeZ9ZbP",
        "outputId": "acb6a07f-a80f-4ea3-8abb-688371cf5c2a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 812.6558... Val Loss: 848.0257\n",
            "Epoch: 1/20... Step: 20... Loss: 1769.9945... Val Loss: 1834.5202\n",
            "Epoch: 1/20... Step: 30... Loss: 1901.9198... Val Loss: 1774.6995\n",
            "Epoch: 1/20... Step: 40... Loss: 1335.9194... Val Loss: 1288.4412\n",
            "Epoch: 1/20... Step: 50... Loss: 906.2190... Val Loss: 1010.4716\n",
            "Epoch: 1/20... Step: 60... Loss: 683.6893... Val Loss: 682.7831\n",
            "Epoch: 1/20... Step: 70... Loss: 498.5186... Val Loss: 506.0106\n",
            "Epoch: 2/20... Step: 80... Loss: 429.9596... Val Loss: 450.0662\n",
            "Epoch: 2/20... Step: 90... Loss: 386.6889... Val Loss: 399.9857\n",
            "Epoch: 2/20... Step: 100... Loss: 297.9394... Val Loss: 231.7064\n",
            "Epoch: 2/20... Step: 110... Loss: 270.1555... Val Loss: 255.6065\n",
            "Epoch: 2/20... Step: 120... Loss: 261.2060... Val Loss: 238.6037\n",
            "Epoch: 2/20... Step: 130... Loss: 263.0961... Val Loss: 285.7343\n",
            "Epoch: 2/20... Step: 140... Loss: 283.4082... Val Loss: 281.4579\n",
            "Epoch: 2/20... Step: 150... Loss: 311.7438... Val Loss: 291.9000\n",
            "Epoch: 3/20... Step: 160... Loss: 299.5562... Val Loss: 317.0892\n",
            "Epoch: 3/20... Step: 170... Loss: 334.9258... Val Loss: 357.8176\n",
            "Epoch: 3/20... Step: 180... Loss: 298.0789... Val Loss: 342.8951\n",
            "Epoch: 3/20... Step: 190... Loss: 338.6444... Val Loss: 318.3010\n",
            "Epoch: 3/20... Step: 200... Loss: 297.9655... Val Loss: 294.9051\n",
            "Epoch: 3/20... Step: 210... Loss: 306.4957... Val Loss: 258.9822\n",
            "Epoch: 3/20... Step: 220... Loss: 332.5635... Val Loss: 352.9458\n",
            "Epoch: 3/20... Step: 230... Loss: 339.2971... Val Loss: 376.7607\n",
            "Epoch: 4/20... Step: 240... Loss: 348.7433... Val Loss: 347.0373\n",
            "Epoch: 4/20... Step: 250... Loss: 389.9725... Val Loss: 400.7408\n",
            "Epoch: 4/20... Step: 260... Loss: 332.7624... Val Loss: 365.5578\n",
            "Epoch: 4/20... Step: 270... Loss: 417.8680... Val Loss: 415.4792\n",
            "Epoch: 4/20... Step: 280... Loss: 343.9188... Val Loss: 362.2721\n",
            "Epoch: 4/20... Step: 290... Loss: 359.4338... Val Loss: 379.1172\n",
            "Epoch: 4/20... Step: 300... Loss: 373.8820... Val Loss: 374.1767\n",
            "Epoch: 5/20... Step: 310... Loss: 412.3317... Val Loss: 380.6210\n",
            "Epoch: 5/20... Step: 320... Loss: 341.6938... Val Loss: 374.4107\n",
            "Epoch: 5/20... Step: 330... Loss: 344.0975... Val Loss: 343.0356\n",
            "Epoch: 5/20... Step: 340... Loss: 341.1594... Val Loss: 379.3771\n",
            "Epoch: 5/20... Step: 350... Loss: 351.6651... Val Loss: 370.8421\n",
            "Epoch: 5/20... Step: 360... Loss: 350.8865... Val Loss: 388.1291\n",
            "Epoch: 5/20... Step: 370... Loss: 375.2361... Val Loss: 387.7004\n",
            "Epoch: 5/20... Step: 380... Loss: 444.5026... Val Loss: 504.6637\n",
            "Epoch: 6/20... Step: 390... Loss: 382.7253... Val Loss: 412.9138\n",
            "Epoch: 6/20... Step: 400... Loss: 327.9162... Val Loss: 370.2882\n",
            "Epoch: 6/20... Step: 410... Loss: 390.1702... Val Loss: 401.8993\n",
            "Epoch: 6/20... Step: 420... Loss: 365.4212... Val Loss: 402.7917\n",
            "Epoch: 6/20... Step: 430... Loss: 372.9947... Val Loss: 384.7893\n",
            "Epoch: 6/20... Step: 440... Loss: 388.4184... Val Loss: 416.1555\n",
            "Epoch: 6/20... Step: 450... Loss: 347.9319... Val Loss: 356.5000\n",
            "Epoch: 6/20... Step: 460... Loss: 360.1337... Val Loss: 353.6378\n",
            "Epoch: 7/20... Step: 470... Loss: 347.8804... Val Loss: 390.3334\n",
            "Epoch: 7/20... Step: 480... Loss: 381.9963... Val Loss: 414.7563\n",
            "Epoch: 7/20... Step: 490... Loss: 361.3832... Val Loss: 365.1208\n",
            "Epoch: 7/20... Step: 500... Loss: 433.1989... Val Loss: 407.5103\n",
            "Epoch: 7/20... Step: 510... Loss: 259.5213... Val Loss: 296.6097\n",
            "Epoch: 7/20... Step: 520... Loss: 413.1186... Val Loss: 434.0058\n",
            "Epoch: 7/20... Step: 530... Loss: 445.3275... Val Loss: 451.7301\n",
            "Epoch: 8/20... Step: 540... Loss: 408.8542... Val Loss: 422.5821\n",
            "Epoch: 8/20... Step: 550... Loss: 321.8678... Val Loss: 381.9241\n",
            "Epoch: 8/20... Step: 560... Loss: 404.2125... Val Loss: 415.9012\n",
            "Epoch: 8/20... Step: 570... Loss: 429.2768... Val Loss: 407.4817\n",
            "Epoch: 8/20... Step: 580... Loss: 381.2373... Val Loss: 406.9833\n",
            "Epoch: 8/20... Step: 590... Loss: 460.6536... Val Loss: 426.0255\n",
            "Epoch: 8/20... Step: 600... Loss: 352.1188... Val Loss: 346.4990\n",
            "Epoch: 8/20... Step: 610... Loss: 410.2009... Val Loss: 430.0761\n",
            "Epoch: 9/20... Step: 620... Loss: 395.3002... Val Loss: 390.7717\n",
            "Epoch: 9/20... Step: 630... Loss: 427.1335... Val Loss: 433.3303\n",
            "Epoch: 9/20... Step: 640... Loss: 449.6991... Val Loss: 432.8995\n",
            "Epoch: 9/20... Step: 650... Loss: 460.9588... Val Loss: 391.9642\n",
            "Epoch: 9/20... Step: 660... Loss: 372.2519... Val Loss: 389.1167\n",
            "Epoch: 9/20... Step: 670... Loss: 474.9664... Val Loss: 476.3497\n",
            "Epoch: 9/20... Step: 680... Loss: 369.6129... Val Loss: 354.6604\n",
            "Epoch: 9/20... Step: 690... Loss: 375.7074... Val Loss: 418.1007\n",
            "Epoch: 10/20... Step: 700... Loss: 343.7269... Val Loss: 380.0001\n",
            "Epoch: 10/20... Step: 710... Loss: 443.6717... Val Loss: 474.6750\n",
            "Epoch: 10/20... Step: 720... Loss: 410.4016... Val Loss: 397.5412\n",
            "Epoch: 10/20... Step: 730... Loss: 381.3627... Val Loss: 423.1325\n",
            "Epoch: 10/20... Step: 740... Loss: 395.7923... Val Loss: 427.4004\n",
            "Epoch: 10/20... Step: 750... Loss: 495.8588... Val Loss: 481.3023\n",
            "Epoch: 10/20... Step: 760... Loss: 471.7494... Val Loss: 490.9148\n",
            "Epoch: 10/20... Step: 770... Loss: 475.5047... Val Loss: 507.1375\n",
            "Epoch: 11/20... Step: 780... Loss: 388.6508... Val Loss: 379.5328\n",
            "Epoch: 11/20... Step: 790... Loss: 436.8093... Val Loss: 422.2777\n",
            "Epoch: 11/20... Step: 800... Loss: 426.9101... Val Loss: 466.1796\n",
            "Epoch: 11/20... Step: 810... Loss: 512.5692... Val Loss: 487.1559\n",
            "Epoch: 11/20... Step: 820... Loss: 399.8172... Val Loss: 405.3140\n",
            "Epoch: 11/20... Step: 830... Loss: 386.4767... Val Loss: 404.2185\n",
            "Epoch: 11/20... Step: 840... Loss: 426.0668... Val Loss: 460.6227\n",
            "Epoch: 12/20... Step: 850... Loss: 397.4835... Val Loss: 409.3878\n",
            "Epoch: 12/20... Step: 860... Loss: 422.4449... Val Loss: 466.8957\n",
            "Epoch: 12/20... Step: 870... Loss: 446.6561... Val Loss: 480.2311\n",
            "Epoch: 12/20... Step: 880... Loss: 388.2639... Val Loss: 402.9908\n",
            "Epoch: 12/20... Step: 890... Loss: 450.6798... Val Loss: 444.2094\n",
            "Epoch: 12/20... Step: 900... Loss: 332.4914... Val Loss: 371.1592\n",
            "Epoch: 12/20... Step: 910... Loss: 393.4926... Val Loss: 402.6020\n",
            "Epoch: 12/20... Step: 920... Loss: 364.1271... Val Loss: 367.0460\n",
            "Epoch: 13/20... Step: 930... Loss: 366.4621... Val Loss: 400.0907\n",
            "Epoch: 13/20... Step: 940... Loss: 356.1022... Val Loss: 354.0187\n",
            "Epoch: 13/20... Step: 950... Loss: 416.4698... Val Loss: 390.2378\n",
            "Epoch: 13/20... Step: 960... Loss: 368.6192... Val Loss: 381.7279\n",
            "Epoch: 13/20... Step: 970... Loss: 375.6889... Val Loss: 390.1709\n",
            "Epoch: 13/20... Step: 980... Loss: 417.5610... Val Loss: 393.0983\n",
            "Epoch: 13/20... Step: 990... Loss: 420.1452... Val Loss: 464.8881\n",
            "Epoch: 13/20... Step: 1000... Loss: 445.2142... Val Loss: 475.7816\n",
            "Epoch: 14/20... Step: 1010... Loss: 465.3329... Val Loss: 482.0181\n",
            "Epoch: 14/20... Step: 1020... Loss: 484.7348... Val Loss: 472.4670\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-e584209135a3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msupernet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-8a1b8ad3e479>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reset to train mode after iterationg through validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}